title,url,content,content_cn,title_cn,Summary
OpenAI's surprise new o3-powered 'Deep Research' mode shows the power of the AI agent era,https://venturebeat.com/ai/openais-surprise-new-o3-powered-deep-research-shows-the-power-of-the-ai-agent-era/,"In case you missed it in favor of the Grammy Awards last night, OpenAI surprised the world late Sunday evening with the announcement of its new “Deep Research” modality, an AI agent available to ChatGPT Pro subscription plan ($200/month) users that’s designed to save humans hours by researching, well, “deeply” and expansively across the web for given topics and compiling professional quality reports across specialized domains from business to science, medicine, marketing and more.
Users of ChatGPT Pro (and soon, ChatGPT Plus, Team, Enterprise and Edu) in the U.S. will be able to access Deep Research by clicking on the option underneath the prompt entry/compose bar at the bottom of the ChatGPT website and apps.
Sam Altman, CEO of OpenAI, described the feature in a series of posts on his personal account on the social network X as “like a superpower; experts on demand!” He added, “It is really good, and can do tasks that would take hours/days and cost hundreds of dollars.”
Deep Research builds on OpenAI’s O Series of reasoning models, specifically leveraging the soon-to-be-released full o3 model (a smaller and less powerful model, o3-mini, was just launched on Friday). The full o3 model can analyze vast amounts of information and integrate text, PDFs, and images into a cohesive analysis.
In a livestream posted to YouTube and available for replay on demand, Mark Chen, OpenAI’s Head of Frontiers Research, explained that “Deep Research is a model that does multi-step research on the internet. It discovers content, synthesizes content, and reasons about this content, adapting its plan as it uncovers more and more information.”
Chen further highlighted the innovation’s importance to OpenAI’s vision: “This is core to our AGI roadmap. Our ultimate aspiration is a model that can uncover and discover new knowledge for itself.”
The launch of the Deep Research marks the second in OpenAI’s official agents following the launch of its browser and cursor controlling Operator earlier this month. And Joshua Achiam, Head of Mission Alignment at Stargate Command at OpenAI wrote on X, both models can help better define the concept of an “AI agent” — a popular but nebulous term these days among enterprises — well beyond the company or these specific use cases.
“I feel like the term ‘agent’ wandered in the desert for a while,” Achaim wrote. “It did not have grounding or examples to point to. But agents like Operator or Deep Research give some shape to this concept. An agent is a general purpose AI that does one or more tool-using workflows for you.”
Deep Research has set new benchmarks for accuracy and reasoning.
Isa Fulford, a member of OpenAI’s research team, shared in the YouTube livestream that the model achieves “a new high of 26.6% accuracy” on “Humanity’s Last Exam” a relatively new AI benchmark designed to be the most difficult for any AI model (or human, for that matter) to complete, covering 3,000 questions across 100 different subjects, such as translating ancient inscriptions on archaeological finds.
Moreover, its ability to browse the web, reason dynamically, and cite sources precisely sets it apart from earlier AI tools.
“The model was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks,” Fulford said. “It learned to plan and execute multi-step trajectories, reacting to real-time information and backtracking when necessary.”
A standout feature of Deep Research is its capacity to handle tasks that would otherwise take humans hours or even days.
During the announcement, Chen explained that “Deep Research generates outputs that resemble a comprehensive, fully cited research paper—something that an analyst or expert in the field might produce.”
The use cases for Deep Research are as diverse as they are impactful.
The official OpenAI account on X stated it was “built for people who do intensive knowledge work in areas like finance, science, policy & engineering and need thorough & reliable research.”
It also appears valuable for consumers seeking personalized recommendations or conducting detailed product research, according to examples shared by OpenAI on its official Deep Research announcement blog post, which includes a detailed research assessment of the best snowboard for someone to buy.
Altman summarized the tool’s versatility, writing, “Give it a try on your hardest work task that can be solved just by using the internet and see what happens.”
Felipe Millon, OpenAI’s Government Go-to-Market lead, shared a deeply personal account of how Deep Research impacted his family. Writing in a series of posts on X, he described his wife’s battle with bilateral breast cancer and how the AI tool became an unexpected ally.
“At the end of October, my wife was diagnosed with bilateral breast cancer. Overnight, our world turned upside down,” Millon wrote.
After a double mastectomy and chemotherapy, the couple faced a critical decision: whether or not to pursue radiation therapy. The situation was fraught with uncertainty, as even their specialists provided mixed recommendations. “For her specific case, it’s completely in a gray area,” Millon explained. “We felt stuck.”
Having preview access to Deep Research, Millon decided to upload his wife’s surgical pathology report and ask whether radiation would be beneficial. “What happened next was mind-blowing,” he wrote. “It didn’t just confirm what our oncologists mentioned—it went deeper. It cited studies I’d never heard of and adapted when we added details like her age and genetic factors.”
The specific prompt he used was:
“Read the surgical pathology report (attached) containing information about the bilateral breast cancer. Then research whether radiation would be indicated for this patient after 6 rounds of TCHP chemotherapy, based on the type of breast cancer. I want to understand the pros and cons of radiation for this patient, how likely it would be to reduce chances of recurrence, and whether the benefits outweigh the potential long-term risks.”
Millon and his wife fact-checked each study cited by the model, finding them to be accurate and highly relevant. “We’re seeing another specialist soon, but we already feel more confident about our decision,” he wrote. “It gave us peace of mind when we needed it most.”
Deep Research is currently available to Pro users of ChatGPT, with plans to expand to the Plus and Team tiers, followed by Enterprise and education markets.
As Chen cautioned, “It’s still possible that it will hallucinate, so when you’re making reports, make sure to check the sources yourself.”
The model’s ability to think autonomously for extended periods also makes it resource-intensive, and OpenAI is currently working on optimizing its performance for broader accessibility.
OpenAI has also hinted at future integrations with custom datasets, which would allow organizations to leverage the tool for proprietary research.
For Millon, the impact of Deep Research is already clear. “We often talk internally at OpenAI about the moments when you ‘feel the AGI,’ and this was one of them,” he wrote. “This thing is going to change the world.”","In case you missed it in favor of the Grammy Awards last night, OpenAI surprised the world late Sunday evening with the announcement of its new “Deep Research” modality, an AI agent available to ChatGPT Pro subscription plan ($200/month) users that’s designed to save humans hours by researching, well, “deeply” and expansively across the web for given topics and compiling professional quality reports across specialized domains from business to science, medicine, marketing and more.
Users of ChatGPT Pro (and soon, ChatGPT Plus, Team, Enterprise and Edu) in the U.S. will be able to access Deep Research by clicking on the option underneath the prompt entry/compose bar at the bottom of the ChatGPT website and apps.
Sam Altman, CEO of OpenAI, described the feature in a series of posts on his personal account on the social network X as “like a superpower; experts on demand!” He added, “It is really good, and can do tasks that would take hours/days and cost hundreds of dollars.”
Deep Research builds on OpenAI’s O Series of reasoning models, specifically leveraging the soon-to-be-released full o3 model (a smaller and less powerful model, o3-mini, was just launched on Friday). The full o3 model can analyze vast amounts of information and integrate text, PDFs, and images into a cohesive analysis.
In a livestream posted to YouTube and available for replay on demand, Mark Chen, OpenAI’s Head of Frontiers Research, explained that “Deep Research is a model that does multi-step research on the internet. It discovers content, synthesizes content, and reasons about this content, adapting its plan as it uncovers more and more information.”
Chen further highlighted the innovation’s importance to OpenAI’s vision: “This is core to our AGI roadmap. Our ultimate aspiration is a model that can uncover and discover new knowledge for itself.”
The launch of the Deep Research marks the second in OpenAI’s official agents following the launch of its browser and cursor controlling Operator earlier this month. And Joshua Achiam, Head of Mission Alignment at Stargate Command at OpenAI wrote on X, both models can help better define the concept of an “AI agent” — a popular but nebulous term these days among enterprises — well beyond the company or these specific use cases.
“I feel like the term ‘agent’ wandered in the desert for a while,” Achaim wrote. “It did not have grounding or examples to point to. But agents like Operator or Deep Research give some shape to this concept. An agent is a general purpose AI that does one or more tool-using workflows for you.”
Deep Research has set new benchmarks for accuracy and reasoning.
Isa Fulford, a member of OpenAI’s research team, shared in the YouTube livestream that the model achieves “a new high of 26.6% accuracy” on “Humanity’s Last Exam” a relatively new AI benchmark designed to be the most difficult for any AI model (or human, for that matter) to complete, covering 3,000 questions across 100 different subjects, such as translating ancient inscriptions on archaeological finds.
Moreover, its ability to browse the web, reason dynamically, and cite sources precisely sets it apart from earlier AI tools.
“The model was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks,” Fulford said. “It learned to plan and execute multi-step trajectories, reacting to real-time information and backtracking when necessary.”
A standout feature of Deep Research is its capacity to handle tasks that would otherwise take humans hours or even days.
During the announcement, Chen explained that “Deep Research generates outputs that resemble a comprehensive, fully cited research paper—something that an analyst or expert in the field might produce.”
The use cases for Deep Research are as diverse as they are impactful.
The official OpenAI account on X stated it was “built for people who do intensive knowledge work in areas like finance, science, policy & engineering and need thorough & reliable research.”
It also appears valuable for consumers seeking personalized recommendations or conducting detailed product research, according to examples shared by OpenAI on its official Deep Research announcement blog post, which includes a detailed research assessment of the best snowboard for someone to buy.
Altman summarized the tool’s versatility, writing, “Give it a try on your hardest work task that can be solved just by using the internet and see what happens.”
Felipe Millon, OpenAI’s Government Go-to-Market lead, shared a deeply personal account of how Deep Research impacted his family. Writing in a series of posts on X, he described his wife’s battle with bilateral breast cancer and how the AI tool became an unexpected ally.
“At the end of October, my wife was diagnosed with bilateral breast cancer. Overnight, our world turned upside down,” Millon wrote.
After a double mastectomy and chemotherapy, the couple faced a critical decision: whether or not to pursue radiation therapy. The situation was fraught with uncertainty, as even their specialists provided mixed recommendations. “For her specific case, it’s completely in a gray area,” Millon explained. “We felt stuck.”
Having preview access to Deep Research, Millon decided to upload his wife’s surgical pathology report and ask whether radiation would be beneficial. “What happened next was mind-blowing,” he wrote. “It didn’t just confirm what our oncologists mentioned—it went deeper. It cited studies I’d never heard of and adapted when we added details like her age and genetic factors.”
The specific prompt he used was:
“Read the surgical pathology report (attached) containing information about the bilateral breast cancer. Then research whether radiation would be indicated for this patient after 6 rounds of TCHP chemotherapy, based on the type of breast cancer. I want to understand the pros and cons of radiation for this patient, how likely it would be to reduce chances of recurrence, and whether the benefits outweigh the potential long-term risks.”
Millon and his wife fact-checked each study cited by the model, finding them to be accurate and highly relevant. “We’re seeing another specialist soon, but we already feel more confident about our decision,” he wrote. “It gave us peace of mind when we needed it most.”
Deep Research is currently available to Pro users of ChatGPT, with plans to expand to the Plus and Team tiers, followed by Enterprise and education markets.
As Chen cautioned, “It’s still possible that it will hallucinate, so when you’re making reports, make sure to check the sources yourself.”
The model’s ability to think autonomously for extended periods also makes it resource-intensive, and OpenAI is currently working on optimizing its performance for broader accessibility.
OpenAI has also hinted at future integrations with custom datasets, which would allow organizations to leverage the tool for proprietary research.
For Millon, the impact of Deep Research is already clear. “We often talk internally at OpenAI about the moments when you ‘feel the AGI,’ and this was one of them,” he wrote. “This thing is going to change the world.”",OpenAI 推出由 o3 驱动的全新“深度研究”模式，展现了人工智能代理时代的力量,OpenAI推出了新的“深度研究”模式，为ChatGPT Pro订阅计划用户提供AI代理，可跨领域深度研究并编制专业质量报告。该模式建立在OpenAI的O系列推理模型基础上，可以分析大量信息并整合文本、PDF和图片，为用于节省时间的深度研究提供了新的标杆。深度研究模型还可以针对各种任务进行动态推理和精确引用来源，具有处理需要长时间或甚至几天才能完成的任务的能力。Felipe Millon分享了他妻子患双侧乳腺癌的亲身经历，使用Deep Research来获得决定性的疾病治疗建议，大大提高了他们的抉择信心。这一工具将逐步拓展到更多用户和市场，OpenAI表示这将是一项改变世界的技术。
Sam Altman admits OpenAI was 'on the wrong side of history' in open source debate,https://venturebeat.com/ai/sam-altman-admits-openai-was-on-the-wrong-side-of-history-in-open-source-debate/,"Sam Altman, CEO of OpenAI, made a striking admission on Friday that his company has been “on the wrong side of history” regarding open-source AI, signaling a potential seismic shift in strategy as competition from China intensifies and efficient open models gain traction.
The candid acknowledgment came during a Reddit “Ask Me Anything” session, just days after Chinese AI firm DeepSeek rattled global markets with its open source R1 model that claims comparable performance to OpenAI’s systems at a fraction of the cost.
“Yes, we are discussing [releasing model weights],” Altman wrote. “I personally think we have been on the wrong side of history here and need to figure out a different open source strategy.” He noted that not everyone at OpenAI shares his view and it isn’t the company’s current highest priority.
The statement represents a remarkable departure from OpenAI’s increasingly proprietary approach in recent years, which has drawn criticism from some AI researchers and former allies, most notably Elon Musk, who is suing the company for allegedly betraying its original open source mission.
Altman’s comments come amid market turmoil triggered by DeepSeek’s emergence. The Chinese company’s claims of building advanced AI models for just $5.6 million in training costs (although total development costs are likely much higher) sent Nvidia’s stock plummeting, wiping out nearly $600 billion in market value — the largest single-day drop for any U.S. company in history.
“We will produce better models, but we will maintain less of a lead than we did in previous years,” Altman acknowledged in the same “Ask Me Anything,” addressing DeepSeek’s impact directly.
DeepSeek’s breakthrough, whether or not its specific claims prove accurate, has highlighted shifting dynamics in AI development. The company says it achieved its results using only 2,000 Nvidia H800 GPUs — far fewer than the estimated 10,000-plus chips typically deployed by major AI labs.
This approach suggests that algorithmic innovation and architectural optimization might matter more than raw computing power. The revelation threatens not just OpenAI’s technical strategy, but its entire business model built on exclusive access to massive computational resources.
However, DeepSeek’s rise has also intensified national security concerns. The company stores user data on servers in mainland China, where it could be subject to government access. Several U.S. agencies have already moved to restrict its use, with NASA becoming the latest to block the application, citing “security and privacy concerns.”
OpenAI’s potential pivot to open source would mark a return to its roots. The company was founded as a non-profit in 2015 with the mission of ensuring artificial general intelligence (AGI) benefits humanity. However, its transition to a “capped-profit” model and increasingly closed approach has drawn criticism from open source advocates.
“The correct reading is: ‘Open source models are surpassing proprietary ones,'” Meta’s chief AI scientist Yann LeCun wrote on LinkedIn, responding to DeepSeek’s emergence. “They came up with new ideas and built them on top of other people’s work. Because their work is published and open source, everyone can profit from it. That is the power of open research and open source.”
While Altman’s comments suggest a strategic shift may be coming, he emphasized that open source isn’t currently OpenAI’s top priority. This hesitation reflects the complex reality facing AI leaders: balancing innovation, security and commercialization in an increasingly multipolar AI world.
The stakes extend far beyond OpenAI’s bottom line. The company’s decision could reshape the entire AI ecosystem. Open-sourcing key models could accelerate innovation and democratize access, but it might also complicate efforts to ensure AI safety and security — core tenets of OpenAI’s mission.
The timing of Altman’s admission, coming after DeepSeek’s market shock rather than before it, suggests that OpenAI may be reacting to market forces rather than leading them. This reactive stance marks a striking role reversal for a company that has long positioned itself as AI’s north star.
As the dust settles from DeepSeek’s debut, one thing becomes clear: The real disruption isn’t just about technology or market value — it’s about challenging the assumption that closely guarded AI models are the surest path to AGI. In that light, Altman’s admission might be less about being on the wrong side of history and more about recognizing that history itself has changed course.","OpenAI 首席执行官 Sam Altman 周五承认，他的公司在开源 AI 方面“站在了历史的错误一边”，这表明，随着来自中国的竞争加剧以及高效开放模型的普及，OpenAI 的战略可能会发生重大转变。
几天前，中国 AI 公司 DeepSeek 推出了开源 R1 模型，震惊了全球市场，该模型声称其性能可与 OpenAI 的系统相媲美，但成本仅为后者的一小部分。
“是的，我们正在讨论 [发布模型权重]，”Altman 写道。“我个人认为，我们在这里站在了历史的错误一边，需要制定不同的开源战略。”他指出，OpenAI 并非所有人都认同他的观点，这也不是公司当前的首要任务。
该声明表明，OpenAI 近年来日益专有的做法发生了显著变化，这种做法引起了一些人工智能研究人员和前盟友的批评，其中最著名的是埃隆·马斯克，他起诉该公司背叛了其最初的开源使命。
Altman 的言论发表之际，DeepSeek 的出现引发了市场动荡。这家中国公司声称仅以 560 万美元的培训成本（尽管总开发成本可能要高得多）就能建立先进的人工智能模型，这导致 Nvidia 的股价暴跌，市值蒸发近 6000 亿美元——这是美国公司历史上最大的单日跌幅。
Altman 在同一期“问我任何事”中承认：“我们将生产出更好的模型，但我们保持的领先优势将比前几年有所减弱”，他直接谈到了 DeepSeek 的影响。
DeepSeek 的突破，无论其具体说法是否准确，都凸显了人工智能发展中不断变化的动态。该公司表示，他们仅使用 2,000 个 Nvidia H800 GPU 就取得了成果——远少于主要 AI 实验室通常部署的 10,000 多个芯片。
这种方法表明，算法创新和架构优化可能比原始计算能力更重要。这一消息不仅威胁到 OpenAI 的技术战略，还威胁到其建立在独家访问大量计算资源基础上的整个商业模式。
然而，DeepSeek 的崛起也加剧了国家安全担忧。该公司将用户数据存储在中国大陆的服务器上，政府可能会访问这些数据。美国几家机构已经开始限制其使用，NASA 成为最新一家以“安全和隐私问题”为由屏蔽该应用程序的机构。
OpenAI 转向开源的潜在举措将标志着其回归本源。该公司成立于 2015 年，是一家非营利组织，其使命是确保通用人工智能 (AGI) 造福人类。然而，其向“利润上限”模式和日益封闭的做法的转变引起了开源倡导者的批评。
“正确的解读是：‘开源模型正在超越专有模型’，”Meta 首席 AI 科学家 Yann LeCun 在 LinkedIn 上回应 DeepSeek 的出现时写道。“他们想出了新的想法，并在其他人的工作基础上建立起来。因为他们的工作是公开的和开源的，所以每个人都可以从中获益。这就是开放研究和开源的力量。”
虽然 Altman 的评论表明战略转变可能即将到来，但他强调开源目前并不是 OpenAI 的首要任务。这种犹豫反映了 AI 领导者面临的复杂现实：在日益多极化的 AI 世界中平衡创新、安全和商业化。
赌注远远超出了 OpenAI 的底线。该公司的决定可能会重塑整个 AI 生态系统。开源关键模型可以加速创新并使访问民主化，但也可能会使确保 AI 安全和保障的努力复杂化——这是 OpenAI 使命的核心原则。
Altman 承认这一点的时间是在 DeepSeek 冲击市场之后，而不是之前，这表明 OpenAI 可能只是在对市场力量做出反应，而不是引领市场力量。这种被动的立场标志着 OpenAI 长期以来将自己定位为人工智能北极星的公司发生了惊人的角色转变。
随着 DeepSeek 的首次亮相尘埃落定，有一点变得清晰：真正的颠覆不仅仅是技术或市场价值，而是挑战了严密保护的人工智能模型是通往 AGI 的最可靠途径这一假设。从这个角度来看，Altman 的承认可能不是站在历史的错误一边，而是认识到历史本身已经改变了方向。",Sam Altman 承认 OpenAI 在开源辩论中“站在了历史的错误一边”,OpenAI公司CEO Sam Altman在周五做出了令人瞩目的承认，表示该公司在开源人工智能方面一直“站错了边”，这表明在来自中国的竞争加剧和高效的开源模式越来越受欢迎的情况下，公司可能会发生战略性转变。在Reddit的“问我任何事”活动中发表了这一坦率的承认，就在中国人工智能公司DeepSeek发布开源R1模型的几天后，这个模型声称在成本只有OpenAI系统的一小部分的情况下具有可比性能，震动了全球市场。尽管目前开源不是OpenAI的首要任务，但Altmant强调，他个人认为他们一直在这方面站错了边，需要找到不同的开源策略。DeepSeek的崛起凸显了人工智能发展中的变化动态，OpenAI的潜在转向开源将标志着它的回归，重塑整个人工智能生态系统。
Applied Digital is harnessing the Nvidia accelerated computing platform to power the next generation of AI workloads,https://venturebeat.com/ai/applied-digital-is-harnessing-the-nvidia-accelerated-computing-platform-to-power-the-next-generation-of-ai-workloads/,"Generative AI applications and ML models are performance-hungry. Today’s workloads — GenAI model training and inferencing, video, image and text data pre- and post-processing, synthetic data generation, SQL and vector database processing, among others — are massive. Next-generation models, like new applications using agentic AI, will require 10 to 20 times more compute to train using significantly more data.
But these huge-scale AI deployments are only as viable as the ability to apply these technologies in an affordable, scalable and resilient manner, says Dave Salvator, director of accelerated computing products at Nvidia.
“Generative AI, and AI in general, is a full-stack problem,” Salvator says. “The chips are obviously at the heart of the platform, but the chip is just the beginning. The full AI stack includes applications and services at the top of the stack, hundreds of libraries in the middle of the stack, and then, of course,  constantly optimizing for the latest, greatest models.”
New technologies and approaches are needed in order to fully unleash the possibilities of accelerated computing in the AI era, including AI platform innovations, renewable power and large-scale liquid cooling, to deliver more affordable, resilient and power-efficient high-performance computing — especially as organizations grapple with the increasing energy challenge.
These data centers can’t be retrofitted — they need instead to be purpose-built, adds Wes Cummins, CEO and chairman of Applied Digital.
“It’s a big lift, upgrading to the type of cooling, power density, electrical, plumbing and the HVAC that needs to be retrofitted. However, the biggest issue goes back to power,” Cummins says. “Efficiency directly translates to lower costs. By maximizing energy efficiency, optimizing space usage and improving infrastructure and equipment utilization in the data center, we can  lower the cost of generating the product out of the hardware.”
Applied Digital is collaborating with Nvidia to deliver the affordable, resilient and power-efficient high-performance computing required to build the AI factory of tomorrow.
The AI factory solves for the end-to-end workflow, helping developers bring AI products to fruition faster. Its compute-intensive processes are significantly more performant, using more power but far more efficiently, so data prep, building models from scratch and pre-training or fine-tuning foundation models are done in a fraction of the time with a fraction of the energy expended.
Models are built faster, more efficiently and more easily than ever with support from truly full-stack solutions. And as advanced generative AI and agentic AI applications start to come to market, even the inference side of deploying is going to become a multi-GPU, multi-node challenge.
Recent Nvidia accelerated computing innovations provide the performance and efficiency needed to address these advanced, accelerated compute requirements, such as the Nvidia Blackwell platform. It uses a lightning speed fabric technology called Nvidia NVLink, which is about seven times faster than PCIe, connecting 72 GPUs in a single domain and can scale up to 576 GPUs to unleash accelerated performance for trillion- and multi-trillion-parameter AI models. Nvidia NVLink Switch technology fully interconnects every GPU, so that any one GPU amongst those 72 can talk to any other at full-line speed, with no bandwidth tradeoff and at low latency. NVLink enables fast all-to-all and all-reduce communications that are extensively used in AI training and inference.
Getting server nodes communicating with each other increasingly becomes a larger part of what can gate performance or allow performance to continue to scale, so really fast performant and configurable networking becomes a critical component of a large system. Nvidia Quantum-2 InfiniBand networking is tailored for AI workloads, providing highly scalable performance with advanced offload engines that reduces training times for large-scale AI models.
“Our goal is to make sure that those scaling efficiencies are as high as they can be, because the more you scale, the more scaled communication becomes a critical part of your performance equation,” Salvatore says.
Keeping high-performance supercomputers running 24/7 can pose a challenge, and failures can be expensive. Interrupted training jobs cost time and money; for deployed applications, if a server goes down and additional servers have to take up the slack, user experience is significantly impacted, and so on.
To address the specific uptime challenges of a GPU-accelerated infrastructure, Blackwell is designed with dedicated engines for reliability, availability and serviceability (RAS). The RAS engine keeps infrastructure managers up to date on server health, and servers self-report any problems so they can be quickly located in a rack of hundreds.
The amount of power necessary to meet the demand for AI infrastructure and drive AI applications is posing a mounting challenge. Applied Digital has a unique approach to solving the issue, which includes “stranded” power, or already-existing energy resources that are untapped or underutilized, and renewable energy. These existing power resources speed up time-to-market while enabling a more ecologically sound method of delivering energy, and will be central to the company’s strategy until more efficient, low-carbon power generation systems become common.
Stranded power is created in North America in two ways: one, when an organization with power-heavy applications goes out of business, such as an aluminum smelter or a steel mill. A large amount of generation and distribution infrastructure was originally put in place to support that factory.
Applied Digital’s primary renewable energy source is wind power, from wind farms in states where land is cheap and the wind is plentiful. Wind turbines are often curtailed because there are frequently not enough sources to push that energy to, and pushing it to the electricity grid can drop prices into the negatives. The company co-locates data centers near these wind farms — in North Dakota, they tap into two gigawatts of wind power feeding into a nearby substation.
“What’s unique about the AI workloads is they’re not as sensitive to network latency to the end user,” Cummins says. “We’re able to be more flexible and actually take the load, the application, directly to the source of power, which we’ve done in multiple locations. Not only do we get to use a large percentage of renewables, but we’re using electricity that would otherwise go unutilized. It creates a lot of local economic benefit and brings a lot of interesting tech jobs into locations in America that have been left behind for the last 20 years.”
Technology advancements in liquid cooling further optimize power efficiency and sustainability. Liquid cooling reduces thermal load and limits power consumption needs. Direct-to-chip liquid-cooled server racks also reduce the need for water resource consumption versus air-cooled or evaporative cooling systems. In 2025, Applied Digital will be deploying liquid cooling to chip at scale. The aim is to drive the PUE metric, or power utilization efficiency, as close to one as possible.
For a PUE of one, 100 percent of the electricity drives the IT workload; anything above one uses that amount of power for cooling and mechanical. Historically, PUE of a very efficient hyperscale data center, depending on the location, would be anywhere from 1.35 to 1.5, while below 1.5 qualifies as a green data center.","Generative AI applications and ML models are performance-hungry. Today’s workloads — GenAI model training and inferencing, video, image and text data pre- and post-processing, synthetic data generation, SQL and vector database processing, among others — are massive. Next-generation models, like new applications using agentic AI, will require 10 to 20 times more compute to train using significantly more data.
But these huge-scale AI deployments are only as viable as the ability to apply these technologies in an affordable, scalable and resilient manner, says Dave Salvator, director of accelerated computing products at Nvidia.
“Generative AI, and AI in general, is a full-stack problem,” Salvator says. “The chips are obviously at the heart of the platform, but the chip is just the beginning. The full AI stack includes applications and services at the top of the stack, hundreds of libraries in the middle of the stack, and then, of course,  constantly optimizing for the latest, greatest models.”
New technologies and approaches are needed in order to fully unleash the possibilities of accelerated computing in the AI era, including AI platform innovations, renewable power and large-scale liquid cooling, to deliver more affordable, resilient and power-efficient high-performance computing — especially as organizations grapple with the increasing energy challenge.
These data centers can’t be retrofitted — they need instead to be purpose-built, adds Wes Cummins, CEO and chairman of Applied Digital.
“It’s a big lift, upgrading to the type of cooling, power density, electrical, plumbing and the HVAC that needs to be retrofitted. However, the biggest issue goes back to power,” Cummins says. “Efficiency directly translates to lower costs. By maximizing energy efficiency, optimizing space usage and improving infrastructure and equipment utilization in the data center, we can  lower the cost of generating the product out of the hardware.”
Applied Digital is collaborating with Nvidia to deliver the affordable, resilient and power-efficient high-performance computing required to build the AI factory of tomorrow.
The AI factory solves for the end-to-end workflow, helping developers bring AI products to fruition faster. Its compute-intensive processes are significantly more performant, using more power but far more efficiently, so data prep, building models from scratch and pre-training or fine-tuning foundation models are done in a fraction of the time with a fraction of the energy expended.
Models are built faster, more efficiently and more easily than ever with support from truly full-stack solutions. And as advanced generative AI and agentic AI applications start to come to market, even the inference side of deploying is going to become a multi-GPU, multi-node challenge.
Recent Nvidia accelerated computing innovations provide the performance and efficiency needed to address these advanced, accelerated compute requirements, such as the Nvidia Blackwell platform. It uses a lightning speed fabric technology called Nvidia NVLink, which is about seven times faster than PCIe, connecting 72 GPUs in a single domain and can scale up to 576 GPUs to unleash accelerated performance for trillion- and multi-trillion-parameter AI models. Nvidia NVLink Switch technology fully interconnects every GPU, so that any one GPU amongst those 72 can talk to any other at full-line speed, with no bandwidth tradeoff and at low latency. NVLink enables fast all-to-all and all-reduce communications that are extensively used in AI training and inference.
Getting server nodes communicating with each other increasingly becomes a larger part of what can gate performance or allow performance to continue to scale, so really fast performant and configurable networking becomes a critical component of a large system. Nvidia Quantum-2 InfiniBand networking is tailored for AI workloads, providing highly scalable performance with advanced offload engines that reduces training times for large-scale AI models.
“Our goal is to make sure that those scaling efficiencies are as high as they can be, because the more you scale, the more scaled communication becomes a critical part of your performance equation,” Salvatore says.
Keeping high-performance supercomputers running 24/7 can pose a challenge, and failures can be expensive. Interrupted training jobs cost time and money; for deployed applications, if a server goes down and additional servers have to take up the slack, user experience is significantly impacted, and so on.
To address the specific uptime challenges of a GPU-accelerated infrastructure, Blackwell is designed with dedicated engines for reliability, availability and serviceability (RAS). The RAS engine keeps infrastructure managers up to date on server health, and servers self-report any problems so they can be quickly located in a rack of hundreds.
The amount of power necessary to meet the demand for AI infrastructure and drive AI applications is posing a mounting challenge. Applied Digital has a unique approach to solving the issue, which includes “stranded” power, or already-existing energy resources that are untapped or underutilized, and renewable energy. These existing power resources speed up time-to-market while enabling a more ecologically sound method of delivering energy, and will be central to the company’s strategy until more efficient, low-carbon power generation systems become common.
Stranded power is created in North America in two ways: one, when an organization with power-heavy applications goes out of business, such as an aluminum smelter or a steel mill. A large amount of generation and distribution infrastructure was originally put in place to support that factory.
Applied Digital’s primary renewable energy source is wind power, from wind farms in states where land is cheap and the wind is plentiful. Wind turbines are often curtailed because there are frequently not enough sources to push that energy to, and pushing it to the electricity grid can drop prices into the negatives. The company co-locates data centers near these wind farms — in North Dakota, they tap into two gigawatts of wind power feeding into a nearby substation.
“What’s unique about the AI workloads is they’re not as sensitive to network latency to the end user,” Cummins says. “We’re able to be more flexible and actually take the load, the application, directly to the source of power, which we’ve done in multiple locations. Not only do we get to use a large percentage of renewables, but we’re using electricity that would otherwise go unutilized. It creates a lot of local economic benefit and brings a lot of interesting tech jobs into locations in America that have been left behind for the last 20 years.”
Technology advancements in liquid cooling further optimize power efficiency and sustainability. Liquid cooling reduces thermal load and limits power consumption needs. Direct-to-chip liquid-cooled server racks also reduce the need for water resource consumption versus air-cooled or evaporative cooling systems. In 2025, Applied Digital will be deploying liquid cooling to chip at scale. The aim is to drive the PUE metric, or power utilization efficiency, as close to one as possible.
For a PUE of one, 100 percent of the electricity drives the IT workload; anything above one uses that amount of power for cooling and mechanical. Historically, PUE of a very efficient hyperscale data center, depending on the location, would be anywhere from 1.35 to 1.5, while below 1.5 qualifies as a green data center.",Applied Digital 正在利用 Nvidia 加速计算平台为下一代 AI 工作负载提供支持,AI应用和机器学习模型对性能要求极高。大规模的工作负载，如GenAI模型训练和推断、视频、图像和文本数据的前后处理、合成数据生成、SQL和向量数据库处理等，都非常庞大。未来新一代模型将需要10到20倍的计算资源来训练，使用更多数据。Nvidia加速计算产品主管Dave Salvator表示，AI的实施必须在成本可承受、可扩展和可靠的基础上。为了充分释放加速计算在AI时代的可能性，需要新的技术和方法，包括AI平台创新、可再生能源和大规模液体冷却等。Applied Digital正在与Nvidia合作，提供所需的经济实惠、可靠和高效能的高性能计算，以构建明天的AI工厂。最近的Nvidia加速计算创新提供了满足这些高级、加速计算需求所需的性能和效率，诸如Nvidia Blackwell平台和Quantum-2 InfiniBand网络等。液体冷却技术的进步进一步优化了功耗效率和可持续性。今年2025年，Applied Digital将在规模上部署液体冷却技术。其目标是将功耗效率指标PUE尽可能接近1，为一个PUE为1的数据中心，100%的电力用于IT工作负荷。
The AI paradox: How tomorrow’s cutting-edge tools can become dangerous cyber threats (and what to do to prepare),https://venturebeat.com/security/the-ai-paradox-how-tomorrows-cutting-edge-tools-can-become-dangerous-cyber-threats-and-how-to-prepare/,"AI is changing the way businesses operate. While much of this shift is positive, it introduces some unique cybersecurity concerns. Next-generation AI applications like agentic AI pose a particularly noteworthy risk to organizations’ security posture.
Agentic AI refers to AI models that can act autonomously, often automating entire roles with little to no human input. Advanced chatbots are among the most prominent examples, but AI agents can also appear in applications like business intelligence, medical diagnoses and insurance adjustments.
In all use cases, this technology combines generative models, natural language processing (NLP) and other machine learning (ML) functions to perform multi-step tasks independently. It’s easy to see the value in such a solution. Understandably, Gartner predicts that one-third of all generative AI interactions will use these agents by 2028.
Agentic AI adoption will surge as businesses seek to complete a larger range of tasks without a larger workforce. As promising as that is, though, giving an AI model so much power has serious cybersecurity implications.
AI agents typically require access to vast amounts of data. Consequently, they are prime targets for cybercriminals, as attackers could focus efforts on a single application to expose a considerable amount of information. It would have a similar effect to whaling — which led to $12.5 billion in losses in 2021 alone — but may be easier, as AI models could be more susceptible than experienced professionals.
Agentic AI’s autonomy is another concern. While all ML algorithms introduce some risks, conventional use cases require human authorization to do anything with their data. Agents, on the other hand, can act without clearance. As a result, any accidental privacy exposures or mistakes like AI hallucinations may slip through without anyone noticing.
This lack of supervision makes existing AI threats like data poisoning all the more dangerous. Attackers can corrupt a model by altering just 0.01% of its training dataset, and doing so is possible with minimal investment. That’s damaging in any context, but a poisoned agent’s faulty conclusions would reach much farther than one where humans review outputs first.
In light of these threats, cybersecurity strategies need to adapt before businesses implement agentic AI applications. Here are four critical steps toward that goal.
The first step is to ensure security and operations teams have full visibility into an AI agent’s workflow. Every task the model completes, each device or app it connects to and all data it can access should be evident. Revealing these factors will make it easier to spot potential vulnerabilities.
Automated network mapping tools may be necessary here. Only 23% of IT leaders say they have full visibility into their cloud environments and 61% use multiple detection tools, leading to duplicate records. Admins must address these issues first to gain the necessary insight into what their AI agents can access.
Once it’s clear what the agent can interact with, businesses must restrict those privileges. The principle of least privilege — which holds that any entity can only see and use what it absolutely needs — is essential.
Any database or application an AI agent can interact with is a potential risk. Consequently, organizations can minimize relevant attack surfaces and prevent lateral movement by limiting these permissions as much as possible. Anything that does not directly contribute to an AI’s value-driving purpose should be off-limits.
Similarly, network admins can prevent privacy breaches by removing sensitive details from the datasets their agentive AI can access. Many AI agents’ work naturally involves private data. More than 50% of generative AI spending will go toward chatbots, which may gather information on customers. However, not all of these details are necessary.
While an agent should learn from past customer interactions, it does not need to store names, addresses or payment details. Programming the system to scrub unnecessary personally identifiable information from AI-accessible data will minimize the damage in the event of a breach.
Businesses need to take care when programming agentive AI, too. Apply it to a single, small use case first and use a diverse team to review the model for signs of bias or hallucinations during training. When it comes time to deploy the agent, roll it out slowly and monitor it for suspicious behavior.
Real-time responsiveness is crucial in this monitoring, as agentive AI’s risks mean any breaches could have dramatic consequences. Thankfully, automated detection and response solutions are highly effective, saving an average of $2.22 million in data breach costs. Organizations can slowly expand their AI agents after a successful trial, but they must continue to monitor all applications.
AI’s rapid advancement holds significant promise for modern businesses, but its cybersecurity risks are rising just as quickly. Enterprises’ cyber defenses must scale up and advance alongside generative AI use cases. Failure to keep up with these changes could cause damage that outweighs the technology’s benefits.
Agentive AI will take ML to new heights, but the same applies to related vulnerabilities. While that does not render this technology too unsafe to invest in, it does warrant extra caution. Businesses must follow these essential security steps as they roll out new AI applications.
Zac Amos is features editor at ReHack.

DataDecisionMakers
Welcome to the VentureBeat community!","AI is changing the way businesses operate. While much of this shift is positive, it introduces some unique cybersecurity concerns. Next-generation AI applications like agentic AI pose a particularly noteworthy risk to organizations’ security posture.
Agentic AI refers to AI models that can act autonomously, often automating entire roles with little to no human input. Advanced chatbots are among the most prominent examples, but AI agents can also appear in applications like business intelligence, medical diagnoses and insurance adjustments.
In all use cases, this technology combines generative models, natural language processing (NLP) and other machine learning (ML) functions to perform multi-step tasks independently. It’s easy to see the value in such a solution. Understandably, Gartner predicts that one-third of all generative AI interactions will use these agents by 2028.
Agentic AI adoption will surge as businesses seek to complete a larger range of tasks without a larger workforce. As promising as that is, though, giving an AI model so much power has serious cybersecurity implications.
AI agents typically require access to vast amounts of data. Consequently, they are prime targets for cybercriminals, as attackers could focus efforts on a single application to expose a considerable amount of information. It would have a similar effect to whaling — which led to $12.5 billion in losses in 2021 alone — but may be easier, as AI models could be more susceptible than experienced professionals.
Agentic AI’s autonomy is another concern. While all ML algorithms introduce some risks, conventional use cases require human authorization to do anything with their data. Agents, on the other hand, can act without clearance. As a result, any accidental privacy exposures or mistakes like AI hallucinations may slip through without anyone noticing.
This lack of supervision makes existing AI threats like data poisoning all the more dangerous. Attackers can corrupt a model by altering just 0.01% of its training dataset, and doing so is possible with minimal investment. That’s damaging in any context, but a poisoned agent’s faulty conclusions would reach much farther than one where humans review outputs first.
In light of these threats, cybersecurity strategies need to adapt before businesses implement agentic AI applications. Here are four critical steps toward that goal.
The first step is to ensure security and operations teams have full visibility into an AI agent’s workflow. Every task the model completes, each device or app it connects to and all data it can access should be evident. Revealing these factors will make it easier to spot potential vulnerabilities.
Automated network mapping tools may be necessary here. Only 23% of IT leaders say they have full visibility into their cloud environments and 61% use multiple detection tools, leading to duplicate records. Admins must address these issues first to gain the necessary insight into what their AI agents can access.
Once it’s clear what the agent can interact with, businesses must restrict those privileges. The principle of least privilege — which holds that any entity can only see and use what it absolutely needs — is essential.
Any database or application an AI agent can interact with is a potential risk. Consequently, organizations can minimize relevant attack surfaces and prevent lateral movement by limiting these permissions as much as possible. Anything that does not directly contribute to an AI’s value-driving purpose should be off-limits.
Similarly, network admins can prevent privacy breaches by removing sensitive details from the datasets their agentive AI can access. Many AI agents’ work naturally involves private data. More than 50% of generative AI spending will go toward chatbots, which may gather information on customers. However, not all of these details are necessary.
While an agent should learn from past customer interactions, it does not need to store names, addresses or payment details. Programming the system to scrub unnecessary personally identifiable information from AI-accessible data will minimize the damage in the event of a breach.
Businesses need to take care when programming agentive AI, too. Apply it to a single, small use case first and use a diverse team to review the model for signs of bias or hallucinations during training. When it comes time to deploy the agent, roll it out slowly and monitor it for suspicious behavior.
Real-time responsiveness is crucial in this monitoring, as agentive AI’s risks mean any breaches could have dramatic consequences. Thankfully, automated detection and response solutions are highly effective, saving an average of $2.22 million in data breach costs. Organizations can slowly expand their AI agents after a successful trial, but they must continue to monitor all applications.
AI’s rapid advancement holds significant promise for modern businesses, but its cybersecurity risks are rising just as quickly. Enterprises’ cyber defenses must scale up and advance alongside generative AI use cases. Failure to keep up with these changes could cause damage that outweighs the technology’s benefits.
Agentive AI will take ML to new heights, but the same applies to related vulnerabilities. While that does not render this technology too unsafe to invest in, it does warrant extra caution. Businesses must follow these essential security steps as they roll out new AI applications.
Zac Amos is features editor at ReHack.

DataDecisionMakers
Welcome to the VentureBeat community!",人工智能悖论：未来的尖端工具如何成为危险的网络威胁（以及需要做好哪些准备）,人工智能正在改变企业经营的方式，尽管这种变化大部分是积极的，但也引入了一些独特的网络安全顾虑。下一代人工智能应用，如主体人工智能，给组织的安全形势带来了尤为引人注目的风险。主体人工智能是指能够自主行动的人工智能模型，通常可以自动化整个角色，几乎没有人类输入。高级聊天机器人是其中最突出的例子之一，但人工智能代理也可能出现在业务智能、医疗诊断和保险调整等应用中。随着企业寻求在不扩大人手的情况下完成更多任务，主体人工智能的采用将激增。尽管这是令人兴奋的，但赋予人工智能模型如此多的权力也带来了严重的网络安全隐患。需要在企业实施主体人工智能应用之前调整网络安全策略，采取四个关键步骤来保障安全。
Clever architecture over raw compute: DeepSeek shatters the ‘bigger is better’ approach to AI development,https://venturebeat.com/ai/clever-architecture-over-raw-compute-deepseek-shatters-the-bigger-is-better-approach-to-ai-development/,"The AI narrative has reached a critical inflection point. The DeepSeek breakthrough — achieving state-of-the-art performance without relying on the most advanced chips — proves what many at NeurIPS in December had already declared: AI’s future isn’t about throwing more compute at problems — it’s about reimagining how these systems work with humans and our environment.
As a Stanford-educated computer scientist who’s witnessed both the promise and perils of AI development, I see this moment as even more transformative than the debut of ChatGPT. We’re entering what some call a “reasoning renaissance.” OpenAI’s o1, DeepSeek’s R1, and others are moving past brute-force scaling toward something more intelligent — and doing so with unprecedented efficiency.
This shift couldn’t be more timely. During his NeurIPS keynote, former OpenAI chief scientist Ilya Sutskever declared that “pretraining will end” because while compute power grows, we’re constrained by finite internet data. DeepSeek’s breakthrough validates this perspective — the China company’s researchers achieved comparable performance to OpenAI’s o1 at a fraction of the cost, demonstrating that innovation, not just raw computing power, is the path forward.
World models are stepping up to fill this gap. World Labs’ recent $230 million raise to build AI systems that understand reality like humans do parallels DeepSeek’s approach, where their R1 model exhibits “Aha!” moments — stopping to re-evaluate problems just as humans do. These systems, inspired by human cognitive processes, promise to transform everything from environmental modeling to human-AI interaction.
We’re seeing early wins: Meta’s recent update to their Ray-Ban smart glasses enables continuous, contextual conversations with AI assistants without wake words, alongside real-time translation. This isn’t just a feature update — it’s a preview of how AI can enhance human capabilities without requiring massive pre-trained models.
However, this evolution comes with nuanced challenges. While DeepSeek has dramatically reduced costs through innovative training techniques, this efficiency breakthrough could paradoxically lead to increased overall resource consumption — a phenomenon known as Jevons Paradox, where technological efficiency improvements often result in increased rather than decreased resource use.
In AI’s case, cheaper training could mean more models being trained by more organizations, potentially increasing net energy consumption. But DeepSeek’s innovation is different: By demonstrating that state-of-the-art performance is possible without cutting-edge hardware, they’re not just making AI more efficient — they’re fundamentally changing how we approach model development.
This shift toward clever architecture over raw computing power could help us escape the Jevons Paradox trap, as the focus moves from “how much compute can we afford?” to “how intelligently can we design our systems?” As UCLA professor Guy Van Den Broeck notes, “The overall cost of language model reasoning is certainly not going down.” The environmental impact of these systems remains substantial, pushing the industry toward more efficient solutions — exactly the kind of innovation DeepSeek represents.
This shift demands new approaches. DeepSeek’s success validates the fact that the future isn’t about building bigger models — it’s about building smarter, more efficient ones that work in harmony with human intelligence and environmental constraints.
Meta’s chief AI scientist Yann LeCun envisions future systems spending days or weeks thinking through complex problems, much like humans do. DeepSeek’s-R1 model, with its ability to pause and reconsider approaches, represents a step toward this vision. While resource-intensive, this approach could yield breakthroughs in climate change solutions, healthcare innovations and beyond. But as Carnegie Mellon’s Ameet Talwalkar wisely cautions, we must question anyone claiming certainty about where these technologies will lead us.
For enterprise leaders, this shift presents a clear path forward. We need to prioritize efficient architecture. One that can:
Here’s what excites me: DeepSeek’s breakthrough proves that we’re moving past the era of “bigger is better” and into something far more interesting. With pretraining hitting its limits and innovative companies finding new ways to achieve more with less, there’s this incredible space opening up for creative solutions.
Smart chains of smaller, specialized agents aren’t just more efficient — they’re going to help us solve problems in ways we never imagined. For startups and enterprises willing to think differently, this is our moment to have fun with AI again, to build something that actually makes sense for both people and the planet.
Kiara Nirghin is an award-winning Stanford technologist, bestselling author and co-founder of Chima.
DataDecisionMakers
Welcome to the VentureBeat community!","人工智能叙事已经到达了一个关键的转折点。DeepSeek 的突破——无需依赖最先进的芯片即可实现最先进的性能——证明了 12 月 NeurIPS 上许多人已经宣称的内容：人工智能的未来不是投入更多计算来解决问题——而是重新想象这些系统如何与人类和我们的环境协同工作。
作为一名斯坦福大学毕业的计算机科学家，我见证了人工智能发展的前景和危险，我认为这一刻比 ChatGPT 的首次亮相更具变革性。我们正在进入一些人所说的“推理复兴”。OpenAI 的 o1、DeepSeek 的 R1 和其他产品正在从蛮力扩展转向更智能的东西——并以前所未有的效率实现。
这种转变再及时不过了。在 NeurIPS 主题演讲中，前 OpenAI 首席科学家 Ilya Sutskever 宣称“预训练将会结束”，因为虽然计算能力在增长，但我们受到有限互联网数据的限制。 DeepSeek 的突破验证了这一观点——这家中国公司的研究人员以极低的成本实现了与 OpenAI 的 o1 相当的性能，这表明创新，而不仅仅是原始的计算能力，才是前进的道路。
世界模型正在加紧填补这一空白。世界实验室最近筹集了 2.3 亿美元，用于构建像人类一样理解现实的人工智能系统，这与 DeepSeek 的方法相似，他们的 R1 模型表现出“啊哈！”时刻——像人类一样停下来重新评估问题。这些系统受到人类认知过程的启发，有望将一切从环境建模转变为人机交互。
我们看到了早期的胜利：Meta 最近对其 Ray-Ban 智能眼镜进行了更新，可以与人工智能助手进行连续的、上下文相关的对话，而无需唤醒词，同时还可以进行实时翻译。这不仅仅是一个功能更新——它是人工智能如何在不需要大量预训练模型的情况下增强人类能力的预览。
然而，这种演变伴随着微妙的挑战。虽然 DeepSeek 通过创新的训练技术大幅降低了成本，但这种效率突破却可能导致总体资源消耗增加——这种现象被称为杰文斯悖论，即技术效率的提高往往会导致资源使用增加而不是减少。
就人工智能而言，更便宜的训练可能意味着更多的模型由更多的组织进行训练，从而可能增加净能耗。但 DeepSeek 的创新有所不同：通过证明无需尖端硬件即可实现最先进的性能，他们不仅提高了人工智能的效率——还从根本上改变了我们开发模型的方式。
这种从原始计算能力转向巧妙架构的转变可以帮助我们摆脱杰文斯悖论陷阱，因为重点从“我们能负担得起多少计算？”转向“我们能多智能地设计我们的系统？”正如加州大学洛杉矶分校教授 Guy Van Den Broeck 所指出的那样，“语言模型推理的总体成本肯定不会下降。”这些系统对环境的影响仍然很大，推动行业走向更高效的解决方案——这正是 DeepSeek 所代表的创新。
这种转变需要新的方法。 DeepSeek 的成功证实了这样一个事实：未来不是要构建更大的模型，而是要构建更智能、更高效的模型，与人类智能和环境约束和谐相处。
Meta 的首席人工智能科学家 Yann LeCun 设想，未来的系统会像人类一样，花费数天或数周时间思考复杂问题。DeepSeek 的 R1 模型具有暂停和重新考虑方法的能力，代表着朝着这一愿景迈出了一步。虽然资源密集型，但这种方法可以在气候变化解决方案、医疗保健创新等领域取得突破。但正如卡内基梅隆大学的 Ameet Talwalkar 明智地警告的那样，我们必须质疑任何声称确定这些技术将引领我们走向何方的人。
对于企业领导者来说，这种转变提供了一条清晰的前进道路。我们需要优先考虑高效的架构。这种架构可以：
让我兴奋的是：DeepSeek 的突破证明我们正在走出“越大越好”的时代，进入一个更有趣的时代。随着预训练达到极限，创新型公司正在寻找新方法以更少的投入实现更多目标，创造性解决方案的潜力无限。
小型专业代理的智能链不仅效率更高，而且还能以我们从未想象过的方式帮助我们解决问题。对于愿意以不同方式思考的初创公司和企业来说，这是我们再次享受人工智能乐趣的时刻，可以打造对人类和地球都有意义的东西。
Kiara Nirghin 是屡获殊荣的斯坦福技术专家、畅销书作家和 Chima 的联合创始人。
DataDecisionMakers
欢迎加入 VentureBeat 社区！",巧妙的架构胜过原始计算：DeepSeek 打破了“越大越好”的 AI 开发方法,AI的发展正处于关键的转折点。DeepSeek取得的突破性进展证明了AI的未来不再是简单地依赖更先进的芯片，而是重新构想这些系统与人类和环境之间的互动方式。该公司的研究人员以较低的成本实现了与OpenAI o1相媲美的性能，验证了创新而非纯粹的计算能力是未来的正确道路。由此，世界级模型不断涌现，这些系统受人类认知过程启发，承诺彻底改变从环境建模到人工智能交互的一切。这种智能架构的转变可能有助于摆脱资源消耗增加的陷阱，从而实现更高效的解决方案。对于企业领导者来说，这种转变为前进指明了清晰的道路。我们需要优先考虑高效的架构，而不是追求更大的模型。现在是我们重拾对人工智能的乐趣，为人类和地球构建有意义的东西的时刻。
Beyond benchmarks: How DeepSeek-R1 and o1 perform on real-world tasks,https://venturebeat.com/ai/beyond-benchmarks-how-deepseek-r1-and-o1-perform-on-real-world-tasks/,"DeepSeek-R1 has surely created a lot of excitement and concern, especially for OpenAI’s rival model o1. So, we put them to test in a side-by-side comparison on a few simple data analysis and market research tasks.
To put the models on equal footing, we used Perplexity Pro Search, which now supports both o1 and R1. Our goal was to look beyond benchmarks and see if the models can actually perform ad hoc tasks that require gathering information from the web, picking out the right pieces of data and performing simple tasks that would require substantial manual effort.
Both models are impressive but make errors when the prompts lack specificity. o1 is slightly better at reasoning tasks but R1’s transparency gives it an edge in cases (and there will be quite a few) where it makes mistakes.
Here is a breakdown of a few of our experiments and the links to the Perplexity pages where you can review the results yourself.
Our first test gauged whether models could calculate returns on investment (ROI). We considered a scenario where the user has invested $140 in the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia, Tesla) on the first day of every month from January to December 2024. We asked the model to calculate the value of the portfolio at the current date.
To accomplish this task, the model would have to pull Mag 7 price information for the first day of each month, split the monthly investment evenly across the stocks ($20 per stock), sum them up and calculate the portfolio value according to the value of the stocks on the current date.
In this task, both models failed. o1 returned a list of stock prices for January 2024 and January 2025 along with a formula to calculate the portfolio value. However, it failed to calculate the correct values and basically said that there would be no ROI. On the other hand, R1 made the mistake of only investing in January 2024 and calculating the returns for January 2025.
However, what was interesting was the models’ reasoning process. While o1 did not provide much details on how it had reached its results, R1’s reasoning traced showed that it did not have the correct information because Perplexity’s retrieval engine had failed to obtain the monthly data for stock prices (many retrieval-augmented generation applications fail not because of the model lack of abilities but because of bad retrieval). This proved to be an important bit of feedback that led us to the next experiment.
We decided to run the same experiment as before, but instead of prompting the model to retrieve the information from the web, we decided to provide it in a text file. For this, we copy-pasted stock monthly data for each stock from Yahoo! Finance into a text file and gave it to the model. The file contained the name of each stock plus the HTML table that contained the price for the first day of each month from January to December 2024 and the last recorded price. The data was not cleaned to reduce the manual effort and test whether the model could pick the right parts from the data.
Again, both models failed to provide the right answer. o1 seemed to have extracted the data from the file, but suggested the calculation be done manually in a tool like Excel. The reasoning trace was very vague and did not contain any useful information to troubleshoot the model. R1 also failed and didn’t provide an answer, but the reasoning trace contained a lot of useful information.
For example, it was clear that the model had correctly parsed the HTML data for each stock and was able to extract the correct information. It had also been able to do the month-by-month calculation of investments, sum them and calculate the final value according to the latest stock price in the table. However, that final value remained in its reasoning chain and failed to make it into the final answer. The model had also been confounded by a row in the Nvidia chart that had marked the company’s 10:1 stock split on June 10, 2024, and ended up miscalculating the final value of the portfolio.
Again, the real differentiator was not the result itself, but the ability to investigate how the model arrived at its response. In this case, R1 provided us with a better experience, allowing us to understand the model’s limitations and how we can reformulate our prompt and format our data to get better results in the future.
Another experiment we carried out required the model to compare the stats of four leading NBA centers and determine which one had the best improvement in field goal percentage (FG%) from the 2022/2023 to the 2023/2024 seasons. This task required the model to do multi-step reasoning over different data points. The catch in the prompt was that it included Victor Wembanyama, who just entered the league as a rookie in 2023.
The retrieval for this prompt was much easier, since player stats are widely reported on the web and are usually included in their Wikipedia and NBA profiles. Both models answered correctly (it’s Giannis in case you were curious), although depending on the sources they used, their figures were a bit different. However, they did not realize that Wemby did not qualify for the comparison and gathered other stats from his time in the European league.
In its answer, R1 provided a better breakdown of the results with a comparison table along with links to the sources it used for its answer. The added context enabled us to correct the prompt. After we modified the prompt specifying that we were looking for FG% from NBA seasons, the model correctly ruled out Wemby from the results.
Reasoning models are powerful tools, but still have a ways to go before they can be fully trusted with tasks, especially as other components of large language model (LLM) applications continue to evolve. From our experiments, both o1 and R1 can still make basic mistakes. Despite showing impressive results, they still need a bit of handholding to give accurate results.
Ideally, a reasoning model should be able to explain to the user when it lacks information for the task. Alternatively, the reasoning trace of the model should be able to guide users to better understand mistakes and correct their prompts to increase the accuracy and stability of the model’s responses. In this regard, R1 had the upper hand. Hopefully, future reasoning models, including OpenAI’s upcoming o3 series, will provide users with more visibility and control.","DeepSeek-R1 has surely created a lot of excitement and concern, especially for OpenAI’s rival model o1. So, we put them to test in a side-by-side comparison on a few simple data analysis and market research tasks.
To put the models on equal footing, we used Perplexity Pro Search, which now supports both o1 and R1. Our goal was to look beyond benchmarks and see if the models can actually perform ad hoc tasks that require gathering information from the web, picking out the right pieces of data and performing simple tasks that would require substantial manual effort.
Both models are impressive but make errors when the prompts lack specificity. o1 is slightly better at reasoning tasks but R1’s transparency gives it an edge in cases (and there will be quite a few) where it makes mistakes.
Here is a breakdown of a few of our experiments and the links to the Perplexity pages where you can review the results yourself.
Our first test gauged whether models could calculate returns on investment (ROI). We considered a scenario where the user has invested $140 in the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia, Tesla) on the first day of every month from January to December 2024. We asked the model to calculate the value of the portfolio at the current date.
To accomplish this task, the model would have to pull Mag 7 price information for the first day of each month, split the monthly investment evenly across the stocks ($20 per stock), sum them up and calculate the portfolio value according to the value of the stocks on the current date.
In this task, both models failed. o1 returned a list of stock prices for January 2024 and January 2025 along with a formula to calculate the portfolio value. However, it failed to calculate the correct values and basically said that there would be no ROI. On the other hand, R1 made the mistake of only investing in January 2024 and calculating the returns for January 2025.
However, what was interesting was the models’ reasoning process. While o1 did not provide much details on how it had reached its results, R1’s reasoning traced showed that it did not have the correct information because Perplexity’s retrieval engine had failed to obtain the monthly data for stock prices (many retrieval-augmented generation applications fail not because of the model lack of abilities but because of bad retrieval). This proved to be an important bit of feedback that led us to the next experiment.
We decided to run the same experiment as before, but instead of prompting the model to retrieve the information from the web, we decided to provide it in a text file. For this, we copy-pasted stock monthly data for each stock from Yahoo! Finance into a text file and gave it to the model. The file contained the name of each stock plus the HTML table that contained the price for the first day of each month from January to December 2024 and the last recorded price. The data was not cleaned to reduce the manual effort and test whether the model could pick the right parts from the data.
Again, both models failed to provide the right answer. o1 seemed to have extracted the data from the file, but suggested the calculation be done manually in a tool like Excel. The reasoning trace was very vague and did not contain any useful information to troubleshoot the model. R1 also failed and didn’t provide an answer, but the reasoning trace contained a lot of useful information.
For example, it was clear that the model had correctly parsed the HTML data for each stock and was able to extract the correct information. It had also been able to do the month-by-month calculation of investments, sum them and calculate the final value according to the latest stock price in the table. However, that final value remained in its reasoning chain and failed to make it into the final answer. The model had also been confounded by a row in the Nvidia chart that had marked the company’s 10:1 stock split on June 10, 2024, and ended up miscalculating the final value of the portfolio.
Again, the real differentiator was not the result itself, but the ability to investigate how the model arrived at its response. In this case, R1 provided us with a better experience, allowing us to understand the model’s limitations and how we can reformulate our prompt and format our data to get better results in the future.
Another experiment we carried out required the model to compare the stats of four leading NBA centers and determine which one had the best improvement in field goal percentage (FG%) from the 2022/2023 to the 2023/2024 seasons. This task required the model to do multi-step reasoning over different data points. The catch in the prompt was that it included Victor Wembanyama, who just entered the league as a rookie in 2023.
The retrieval for this prompt was much easier, since player stats are widely reported on the web and are usually included in their Wikipedia and NBA profiles. Both models answered correctly (it’s Giannis in case you were curious), although depending on the sources they used, their figures were a bit different. However, they did not realize that Wemby did not qualify for the comparison and gathered other stats from his time in the European league.
In its answer, R1 provided a better breakdown of the results with a comparison table along with links to the sources it used for its answer. The added context enabled us to correct the prompt. After we modified the prompt specifying that we were looking for FG% from NBA seasons, the model correctly ruled out Wemby from the results.
Reasoning models are powerful tools, but still have a ways to go before they can be fully trusted with tasks, especially as other components of large language model (LLM) applications continue to evolve. From our experiments, both o1 and R1 can still make basic mistakes. Despite showing impressive results, they still need a bit of handholding to give accurate results.
Ideally, a reasoning model should be able to explain to the user when it lacks information for the task. Alternatively, the reasoning trace of the model should be able to guide users to better understand mistakes and correct their prompts to increase the accuracy and stability of the model’s responses. In this regard, R1 had the upper hand. Hopefully, future reasoning models, including OpenAI’s upcoming o3 series, will provide users with more visibility and control.",超越基准：DeepSeek-R1 和 o1 在实际任务中的表现如何,深度搜索-R1的发布引起了很大的兴奋和关注，尤其是对OpenAI的竞争对手o1造成了影响。我们对这两个模型在数据分析和市场研究任务方面进行了比较测试，发现o1在推理任务上略胜一筹，但R1在透明度方面更具优势。在一项实验中，这两个模型都未能正确计算投资回报率。R1在演示链中提供了更多有用信息，尽管失败，而o1则提供了模糊的推理链。未来的推理模型需要更大的透明度和控制性，以便更好地理解和纠正错误，提高准确性和稳定性。希望未来的推理模型能够提供更多的可见性和控制性。
